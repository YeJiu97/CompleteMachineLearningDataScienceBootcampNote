<p>Once your data is all in numerical format, there's one more transformation you'll probably want to do to it.</p><p>It's called <strong>Feature Scaling</strong>.</p><p>In other words, making sure all of your numerical data is on the same scale.</p><p>For example, say you were trying to predict the sale price of cars and the number of kilometres on their odometers varies from 6,000 to 345,000 but the median previous repair cost varies from 100 to 1,700. A machine learning algorithm may have trouble finding patterns in these wide-ranging variables.</p><p>To fix this, there are two main types of feature scaling.</p><ul><li><p><strong>Normalization </strong>(also called min-max scaling) - This rescales all the numerical values to between 0 and 1, with the lowest value being close to 0 and the highest previous value being close to 1. Scikit-Learn provides functionality for this in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" rel="noopener noreferrer" target="_blank">MinMaxScalar class</a>.</p></li><li><p><strong>Standardization</strong> - This subtracts the mean value from all of the features (so the resulting features have 0 mean). It then scales the features to unit variance (by dividing the feature by the standard deviation). Scikit-Learn provides functionality for this in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener noreferrer" target="_blank">StandardScalar class</a>.</p></li></ul><p>A couple of things to note.</p><ul><li><p>Feature scaling usually isn't required for your target variable. </p></li><li><p>Feature scaling is usually not required with tree-based models (e.g. Random Forest) since they can handle varying features.</p></li></ul><p><strong>Extra reading</strong></p><p>For further information on this topic, I'd suggest the following resources.</p><ul><li><p><a href="https://medium.com/@rahul77349/feature-scaling-why-it-is-required-8a93df1af310" rel="noopener noreferrer" target="_blank">Feature Scaling - why is it required?</a> by Rahul Saini</p></li><li><p><a href="https://benalexkeen.com/feature-scaling-with-scikit-learn/" rel="noopener noreferrer" target="_blank">Feature Scaling with Scikit-Learn</a> by Ben Alex Keen</p></li><li><p><a href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener noreferrer" target="_blank">Feature Scaling for Machine Learning: Understanding the Difference Between Normalization vs. Standardization</a> by Aniruddha Bhandari </p></li></ul><p><strong>Challenge</strong></p><p>After reading up on feature scaling, a good idea would be to practice it on one of the problems you're working on and see how it affects the results. If you find anything interesting, be sure to share it.</p><p>Thank you to Sid and Shubhamai for suggesting resources. If you have anything you think should be added, please let us know.</p>